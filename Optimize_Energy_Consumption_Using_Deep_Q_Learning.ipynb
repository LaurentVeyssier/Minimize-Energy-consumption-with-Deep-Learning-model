{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Artificial Intelligence for Business\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the environment\n",
    "- Use a class to create different Environment objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    # introduce and initialize all paramaters and variables of the environment\n",
    "    def __init__(self, optimal_temperature = [18.0, 24.0], initial_month= 0, \\\n",
    "                 initial_number_users = 10, initial_rate_data = 60):\n",
    "        \n",
    "        self.initial_month = initial_month\n",
    "        \n",
    "        self.monthly_atmospheric_temperatures = [1.0, 5.0, 7.0, 10.0, 11.0, 20.0, \n",
    "                                                 23.0, 24.0, 22.0, 10.0, 5.0, 1.0]\n",
    "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[initial_month]\n",
    "        self.optimal_temperature = optimal_temperature\n",
    "        self.min_temperature = -20\n",
    "        self.max_temperature = 80\n",
    "        \n",
    "        self.min_number_users = 10\n",
    "        self.max_number_users = 100\n",
    "        self.max_update_users = 5\n",
    "        self.initial_number_users = initial_number_users\n",
    "        self.current_number_users = initial_number_users\n",
    "        \n",
    "        self.min_rate_data = 20\n",
    "        self.max_rate_data = 300\n",
    "        self.max_update_data = 10\n",
    "        self.initial_rate_data = initial_rate_data\n",
    "        self.current_rate_data = initial_rate_data\n",
    "        \n",
    "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users \\\n",
    "                                    + 1.25 * self.current_rate_data\n",
    "        self.temperature_ai = self.intrinsic_temperature\n",
    "        self.temperature_noai = (self.optimal_temperature[0] + self.optimal_temperature[1]) / 2.0 # mid of optimal range\n",
    "        \n",
    "        self.total_energy_ai = 0.0\n",
    "        self.total_energy_noai = 0.0\n",
    "        \n",
    "        self.reward = 0.0\n",
    "        self.game_over = 0\n",
    "        self.train = 1        # train or inference mode\n",
    "        \n",
    "    # method to update environment after AI plays an action\n",
    "    def update_env(self, direction, energy_ai, month):\n",
    "        \"\"\" variables:\n",
    "         - direction :  change of temperature by AI incr or decr +1 or -1 \"\"\"\n",
    "        \n",
    "        # GETTING THE REWARD\n",
    "        # Computing the energy spent by the server's cooling system when there is no AI\n",
    "        energy_noai = 0\n",
    "        if (self.temperature_noai < self.optimal_temperature[0]):\n",
    "            energy_noai = self.optimal_temperature[0] - self.temperature_noai\n",
    "            self.temperature_noai = self.optimal_temperature[0]\n",
    "        elif (self.temperature_noai > self.optimal_temperature[1]):\n",
    "            energy_noai = self.temperature_noai - self.optimal_temperature[1]\n",
    "            self.temperature_noai = self.optimal_temperature[1]\n",
    "        # Computing the Reward and Scaling the Reward\n",
    "        self.reward = energy_noai - energy_ai\n",
    "        self.reward = 1e-3 * self.reward\n",
    "        \n",
    "        # GETTING NEXT STATE\n",
    "        # Updating the atmospheric temperature\n",
    "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[month]\n",
    "        # Updating the number of users between the min / max range\n",
    "        self.current_number_users += np.random.randint(-self.max_update_users, self.max_update_users)\n",
    "        if (self.current_number_users > self.max_number_users):\n",
    "            self.current_number_users = self.max_number_users\n",
    "        elif (self.current_number_users < self.min_number_users):\n",
    "            self.current_number_users = self.min_number_users\n",
    "        # Updating the rate of data between the min / max range\n",
    "        self.current_rate_data += np.random.randint(-self.max_update_data, self.max_update_data)\n",
    "        if (self.current_rate_data > self.max_rate_data):\n",
    "            self.current_rate_data = self.max_rate_data\n",
    "        elif (self.current_rate_data < self.min_rate_data):\n",
    "            self.current_rate_data = self.min_rate_data\n",
    "        # Computing the Delta of Intrinsic Temperature\n",
    "        past_intrinsic_temperature = self.intrinsic_temperature       # T° of server before action\n",
    "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users \\\n",
    "                                     + 1.25 * self.current_rate_data  # T° of server updated\n",
    "        delta_intrinsic_temperature = self.intrinsic_temperature - past_intrinsic_temperature\n",
    "        # Computing the Delta of Temperature caused by the AI action\n",
    "        if (direction == -1):\n",
    "            delta_temperature_ai = -energy_ai  # energy cost = abs delta of T° change by assumption\n",
    "        elif (direction == 1):\n",
    "            delta_temperature_ai = energy_ai\n",
    "        # Updating the new Server's Temperature when there is the AI\n",
    "        self.temperature_ai += delta_intrinsic_temperature + delta_temperature_ai\n",
    "        # Updating the new Server's Temperature when there is no AI\n",
    "        self.temperature_noai += delta_intrinsic_temperature\n",
    "        \n",
    "        # GETTING GAME OVER (allows to end of an epoch if T° out of bound during training)\n",
    "        if (self.temperature_ai < self.min_temperature):\n",
    "            if self.train == 1:\n",
    "                self.game_over = 1\n",
    "            else:\n",
    "                self.total_energy_ai += self.optimal_temperature[0] - self.temperature_ai\n",
    "                self.temperature_ai = self.optimal_temperature[0]\n",
    "        elif (self.temperature_ai > self.max_temperature):   \n",
    "            if self.train == 1:\n",
    "                self.game_over = 1\n",
    "            else:\n",
    "                self.total_energy_ai += self.temperature_ai - self.optimal_temperature[1] \n",
    "                self.temperature_ai = self.optimal_temperature[1]\n",
    "        \n",
    "        # UPDATING THE SCORES\n",
    "        self.total_energy_ai += energy_ai\n",
    "        self.total_energy_noai += energy_noai\n",
    "        \n",
    "        # NORMALIZE NEXT STATE (state vector to be fed to neural network)\n",
    "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / \\\n",
    "                                (self.max_temperature - self.min_temperature)\n",
    "        scaled_number_users = (self.current_number_users - self.min_number_users) / \\\n",
    "                              (self.max_number_users - self.min_number_users)\n",
    "        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / \\\n",
    "                           (self.max_rate_data - self.min_rate_data)\n",
    "        # create vector for updated state\n",
    "        next_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])\n",
    "        \n",
    "        return next_state, self.reward, self.game_over \n",
    "    \n",
    "    # METHOD THAT RESETS THE ENVIRONMENT\n",
    "    def reset(self, new_month):\n",
    "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[new_month]\n",
    "        self.initial_month = new_month\n",
    "        self.current_number_users = self.initial_number_users\n",
    "        self.current_rate_data = self.initial_rate_data\n",
    "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users \\\n",
    "                                     + 1.25 * self.current_rate_data\n",
    "        self.temperature_ai = self.intrinsic_temperature\n",
    "        self.temperature_noai = (self.optimal_temperature[0] + self.optimal_temperature[1]) / 2.0\n",
    "        self.total_energy_ai = 0.0\n",
    "        self.total_energy_noai = 0.0\n",
    "        self.reward = 0.0\n",
    "        self.game_over = 0\n",
    "        self.train = 1\n",
    "\n",
    "    # METHOD PROVIDING CURRENT STATE, LAST REWARD AND WHETHER THE GAME IS OVER\n",
    "    def observe(self):\n",
    "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / \\\n",
    "                                (self.max_temperature - self.min_temperature)\n",
    "        scaled_number_users = (self.current_number_users - self.min_number_users) / \\\n",
    "                              (self.max_number_users - self.min_number_users)\n",
    "        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / \\\n",
    "                           (self.max_rate_data - self.min_rate_data)\n",
    "        # calc vector of current state\n",
    "        current_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])\n",
    "        \n",
    "        return current_state, self.reward, self.game_over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Neural network\n",
    "- Fully connected NN with 2 hidden layers (64 then 32 nodes)\n",
    "- Input : state vector (server T°, number of users, rate of data)\n",
    "- Output : Q-values of AI actions to regulate T° ( reduce by 3°C or 1.5°, maintain T°, Incr. by 1.5° or 3°C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain(object):\n",
    "    def __init__(self, learning_rate = 0.001, number_actions = 5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.number_actions = number_actions\n",
    "        states = Input(shape = (3,))\n",
    "        x = Dense(units = 64, activation = 'sigmoid')(states)\n",
    "        #x = Dropout(rate = 0.1)(x)\n",
    "        x = Dense(units = 32, activation = 'sigmoid')(x)\n",
    "        #x = Dropout(rate = 0.1)(x)\n",
    "        q_values = Dense(units = self.number_actions, activation = 'softmax')(x)\n",
    "        \n",
    "        self.model = Model(inputs = states, outputs = q_values)\n",
    "        self.model.compile(loss='mse', optimizer = Adam(lr=self.learning_rate))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Deep Q-Learning with Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    \n",
    "    # INITIALIZE ALL THE PARAMETERS AND VARIABLES OF THE DQN\n",
    "    def __init__(self, max_memory = 100, discount = 0.9):\n",
    "        self.memory = list()\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount     # discount factor used in calculating the targets Q\n",
    "\n",
    "    # METHOD THAT BUILDS THE MEMORY IN EXPERIENCE REPLAY\n",
    "    def remember(self, transition, game_over):\n",
    "        \"\"\"arguments:\n",
    "        transition: tuple of 4 elemnts (current state, action played, reward received, next state)\n",
    "        game_over : 0 or 1\"\"\"\n",
    "        self.memory.append([transition, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]                   # delete first memory element (FIFO)\n",
    "\n",
    "    # CONSTRUCT BATCHES OF INPUTS AND TARGETS BY EXTRACTING TRANSITIONS FROM THE MEMORY\n",
    "    def get_batch(self, model, batch_size = 10):\n",
    "        len_memory = len(self.memory)\n",
    "        num_inputs = self.memory[0][0][0].shape[1]  # select first elmnt of transition tuple, ie shape of state vector\n",
    "        num_outputs = model.output_shape[-1]\n",
    "        \n",
    "        # initialize the batches\n",
    "        inputs = np.zeros((min(len_memory, batch_size), num_inputs))   # typically batch_size x 3\n",
    "        targets = np.zeros((min(len_memory, batch_size), num_outputs)) # typically batch_size x 5\n",
    "        \n",
    "        # extract random transitions from memory and populate input states and outputs Q-values\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size = min(len_memory, batch_size))):\n",
    "            current_state, action, reward, next_state = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "            inputs[i] = current_state\n",
    "            targets[i] = model.predict(current_state)[0]  # predict returns 2 elements, Q-values is first\n",
    "            Q_sa = np.max(targets[i])\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        \n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/100\n",
      "Total Energy spent with an AI: 36\n",
      "Total Energy spent with no AI: 128\n",
      "Epoch: 002/100\n",
      "Total Energy spent with an AI: 14\n",
      "Total Energy spent with no AI: 43\n",
      "Epoch: 003/100\n",
      "Total Energy spent with an AI: 9\n",
      "Total Energy spent with no AI: 14\n",
      "Epoch: 004/100\n",
      "Total Energy spent with an AI: 33\n",
      "Total Energy spent with no AI: 62\n",
      "Epoch: 005/100\n",
      "Total Energy spent with an AI: 33\n",
      "Total Energy spent with no AI: 49\n",
      "Epoch: 006/100\n",
      "Total Energy spent with an AI: 6\n",
      "Total Energy spent with no AI: 64\n",
      "Epoch: 007/100\n",
      "Total Energy spent with an AI: 51\n",
      "Total Energy spent with no AI: 230\n",
      "Epoch: 008/100\n",
      "Total Energy spent with an AI: 24\n",
      "Total Energy spent with no AI: 68\n",
      "Epoch: 009/100\n",
      "Total Energy spent with an AI: 8\n",
      "Total Energy spent with no AI: 16\n",
      "Epoch: 010/100\n",
      "Total Energy spent with an AI: 63\n",
      "Total Energy spent with no AI: 108\n",
      "Epoch: 011/100\n",
      "Total Energy spent with an AI: 9\n",
      "Total Energy spent with no AI: 34\n",
      "Epoch: 012/100\n",
      "Total Energy spent with an AI: 8\n",
      "Total Energy spent with no AI: 20\n",
      "Epoch: 013/100\n",
      "Total Energy spent with an AI: 26\n",
      "Total Energy spent with no AI: 52\n",
      "Epoch: 014/100\n",
      "Total Energy spent with an AI: 20\n",
      "Total Energy spent with no AI: 104\n",
      "Epoch: 015/100\n",
      "Total Energy spent with an AI: 22\n",
      "Total Energy spent with no AI: 52\n",
      "Epoch: 016/100\n",
      "Total Energy spent with an AI: 42\n",
      "Total Energy spent with no AI: 256\n",
      "Epoch: 017/100\n",
      "Total Energy spent with an AI: 15\n",
      "Total Energy spent with no AI: 28\n",
      "Epoch: 018/100\n",
      "Total Energy spent with an AI: 12\n",
      "Total Energy spent with no AI: 44\n",
      "Epoch: 019/100\n",
      "Total Energy spent with an AI: 8\n",
      "Total Energy spent with no AI: 14\n",
      "Epoch: 020/100\n",
      "Total Energy spent with an AI: 20\n",
      "Total Energy spent with no AI: 69\n",
      "Epoch: 021/100\n",
      "Total Energy spent with an AI: 3\n",
      "Total Energy spent with no AI: 11\n",
      "Epoch: 022/100\n",
      "Total Energy spent with an AI: 15\n",
      "Total Energy spent with no AI: 55\n",
      "Epoch: 023/100\n",
      "Total Energy spent with an AI: 6\n",
      "Total Energy spent with no AI: 22\n",
      "Epoch: 024/100\n",
      "Total Energy spent with an AI: 32\n",
      "Total Energy spent with no AI: 58\n",
      "Epoch: 025/100\n",
      "Total Energy spent with an AI: 3\n",
      "Total Energy spent with no AI: 8\n",
      "Epoch: 026/100\n",
      "Total Energy spent with an AI: 8\n",
      "Total Energy spent with no AI: 29\n",
      "Early Stopping\n"
     ]
    }
   ],
   "source": [
    "# Setting seeds for reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "random.seed(12345)\n",
    "\n",
    "# SETTING UP THE PARAMETERS\n",
    "epsilon = .3    # exploration vs exploitation ratio. Here 30% exploration (random selection)\n",
    "number_actions = 5\n",
    "direction_boundary = (number_actions - 1) / 2   # boundary separating direction of T° change actions\n",
    "number_epochs = 100\n",
    "max_memory = 3000\n",
    "batch_size = 512\n",
    "temperature_step = 1.5\n",
    "\n",
    "# BUILDING THE ENVIRONMENT BY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
    "env = Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, \\\n",
    "                  initial_number_users = 20, initial_rate_data = 30)\n",
    "\n",
    "# BUILDING THE NEURAL NETWORK OBJECT USING BRAIN CLASS\n",
    "brain = Brain(learning_rate = 0.00001, number_actions = number_actions)\n",
    "\n",
    "# BUILDING THE DQN MODEL\n",
    "dqn = DQN(max_memory = max_memory, discount = 0.9)\n",
    "\n",
    "# CHOOSING THE MODE\n",
    "train = True\n",
    "\n",
    "# TRAINING THE AI\n",
    "env.train = train\n",
    "model = brain.model\n",
    "early_stopping = True\n",
    "patience = 10\n",
    "best_total_reward = -np.inf\n",
    "patience_count = 0\n",
    "\n",
    "if (env.train):\n",
    "    \n",
    "    # STARTING THE LOOP OVER ALL THE EPOCHS (1 Epoch = 5 Months)\n",
    "    for epoch in range(1, number_epochs):\n",
    "        \n",
    "        # INITIALIZING ALL THE VARIABLES OF BOTH THE ENVIRONMENT AND THE TRAINING LOOP\n",
    "        total_reward = 0\n",
    "        loss = 0.\n",
    "        new_month = np.random.randint(0, 12)\n",
    "        env.reset(new_month = new_month)\n",
    "        game_over = False\n",
    "        current_state, _, _ = env.observe()\n",
    "        timestep = 0\n",
    "        \n",
    "        # STARTING THE LOOP OVER ALL THE TIMESTEPS (1 Timestep = 1 Minute) IN ONE EPOCH\n",
    "        while ((not game_over) and timestep <= 5 * 30 * 24 * 60):\n",
    "            \n",
    "            # PLAYING THE NEXT ACTION BY EXPLORATION\n",
    "            if np.random.rand() <= epsilon:   # random pick bw [0,1] below epsilon threshold ?\n",
    "                action = np.random.randint(0, number_actions)  # action bw 0 to 4, boundary = 2\n",
    "                if (action - direction_boundary < 0):\n",
    "                    direction = -1\n",
    "                else:\n",
    "                    direction = 1\n",
    "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
    "            \n",
    "            # PLAYING THE NEXT ACTION BY INFERENCE\n",
    "            else:\n",
    "                q_values = model.predict(current_state)\n",
    "                action = np.argmax(q_values[0])\n",
    "                if (action - direction_boundary < 0):\n",
    "                    direction = -1\n",
    "                else:\n",
    "                    direction = 1\n",
    "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
    "            \n",
    "            # UPDATING THE ENVIRONMENT AND REACHING THE NEXT STATE\n",
    "            next_state, reward, game_over = env.update_env(direction, energy_ai, \\\n",
    "                                                           int(timestep / (30 * 24 * 60)))  # month [1,5]\n",
    "            total_reward += reward\n",
    "            \n",
    "            # STORING THIS NEW TRANSITION INTO THE MEMORY\n",
    "            dqn.remember([current_state, action, reward, next_state], game_over)\n",
    "            \n",
    "            # GATHERING IN TWO SEPARATE BATCHES THE INPUTS AND THE TARGETS\n",
    "            inputs, targets = dqn.get_batch(model, batch_size = batch_size)\n",
    "            \n",
    "            # COMPUTING THE LOSS OVER THE TWO WHOLE BATCHES OF INPUTS AND TARGETS\n",
    "            loss += model.train_on_batch(inputs, targets)  # keras method training a minibatch with gr descent\n",
    "            timestep += 1\n",
    "            current_state = next_state                     # update the current state\n",
    "        \n",
    "        # PRINTING THE TRAINING RESULTS FOR EACH EPOCH\n",
    "        print(\"Epoch: {:03d}/{:03d}\".format(epoch, number_epochs))\n",
    "        print(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))\n",
    "        print(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai))\n",
    "        \n",
    "        # EARLY STOPPING\n",
    "        if (early_stopping):\n",
    "            if (total_reward <= best_total_reward):\n",
    "                patience_count += 1\n",
    "            elif (total_reward > best_total_reward):\n",
    "                best_total_reward = total_reward\n",
    "                patience_count = 0\n",
    "            if (patience_count >= patience):\n",
    "                print(\"Early Stopping\")\n",
    "                break\n",
    "        \n",
    "        # SAVING THE MODEL\n",
    "        model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Energy management model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating one year of energy management...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 518400/518400 [06:08<00:00, 1407.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Energy spent with an AI: 628889\n",
      "Total Energy spent with no AI: 1977614\n",
      "ENERGY SAVED WITH AI: 68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Evaluating one year of energy management...')\n",
    "\n",
    "# BUILDING THE ENVIRONMENT BY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
    "env = Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, \\\n",
    "                  initial_number_users = 20, initial_rate_data = 30)\n",
    "\n",
    "# LOAD PRE-TRAINED MODEL\n",
    "model = load_model(\"model.h5\")\n",
    "\n",
    "# CHOOSING THE MODE\n",
    "train = False\n",
    "\n",
    "# RUNNING 1 YEAR SIMULATION INFERENCE MODE\n",
    "env.train = train\n",
    "current_state, _, _ = env.observe()\n",
    "\n",
    "# STARTING THE LOOP OVER 1 YEAR\n",
    "for timestep in tqdm(range(12 * 30 * 24 * 60)):\n",
    "    q_values = model.predict(current_state)\n",
    "    action = np.argmax(q_values[0])\n",
    "    if (action - direction_boundary < 0):\n",
    "        direction = -1\n",
    "    else:\n",
    "        direction = 1\n",
    "    energy_ai = abs(action - direction_boundary) * temperature_step\n",
    "    # UPDATING ENVIRONMENT AND REACHING THE NEXT STATE\n",
    "    next_state, _, _ = env.update_env(direction, energy_ai, \\\n",
    "                                                int(timestep / (30 * 24 * 60)))  # month [0,11]         \n",
    "    current_state = next_state    # update the current state\n",
    "    \n",
    "# PRINTING THE RESULTS FOR 1 YEAR\n",
    "print(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))\n",
    "print(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai))\n",
    "print(\"ENERGY SAVED WITH AI: {:.0f}%\".format((env.total_energy_noai - env.total_energy_ai)/env.total_energy_noai*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_py37",
   "language": "python",
   "name": "tensorflow_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
